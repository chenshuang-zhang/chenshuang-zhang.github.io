<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ImageNet-D.">
  <meta name="keywords" content="Diffusion models, robustness evaluation, vision-language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object [CVPR 2024 Highlight]</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chenshuang-zhang.github.io/">Chenshuang Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.feipan.info/">Fei Pan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=GdQtWNQAAAAJ">Junmo Kim</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XA8EOlEAAAAJ">In So Kweon</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.columbia.edu/~mcz/">Chengzhi Mao</a><sup>*3,4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KAIST</span>
            <span class="author-block"><sup>2</sup>University of Michigan</span>
            <span class="author-block"><sup>3</sup> McGill University</span>
            <span class="author-block"><sup>4</sup>MILA</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Corresponding author.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.18775"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/chenshuang-zhang/imagenet_d"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/11zTXmg5yNjZwi8bwc541M1h5tPAVGeQc/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                </span>

            <!-- Youtube Link. -->
            <span class="link-block">
            <a href="https://www.youtube.com/watch?v=CQm2oDfCvR8"
                class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="far fa-images"></i>
              </span>
              <span>Youtube</span>
              </a>
            </span>            
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Teaser. -->
      <div class="columns is-centered has-text-justified">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3">Dataset</h2> -->
          <div class="one">
            <div class="two" id='mira_image'>
            <img src="static/images/teaser_figure.png" width="800px" height="2100px" />
            <p>
              Top predictions from CLIP (ViT-L/14) on ImageNet-D. We synthesize the images by changing their background, texture and material. The groundtruth for the images are plunger, spatula, and ladle in order, together with the background (badminton court), texture (freckled), and material (painted). 
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Teaser. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D.  Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models can be an effective source to test vision models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

        <!-- Comparison to prior work. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">ImageNet-D achieves high image fidelity</h2>
            <div class="one">
              <div class="two" id='mira_image'>
              <img src="static/images/test_set_comparison.png" width="800px" height="2100px" />
            </div>
    
            <div class="content has-text-justified">
              <p>
                Examples from  ImageNet-9, Stylized-ImageNet and ImageNet-C and our ImageNet-D. For the second row, we show images from ImageNet-D with different backgrounds, textures and materials orderly. Take the background for example (the two columns on the left), ImageNet-9~\cite{xiao2020noise} generates new images by simply cutting and paste foreground and background from different images, leading to object deformation and dislocation. By contrast, ImageNet-D includes images with diverse backgrounds by diffusion generation, achieving superior visual fidelity. 
              </p>
            </div>
          </div>
        </div>
      </div>


    <!-- Dataset samples. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ImageNet-D samples</h2>
        <div class="one">
          <div class="two" id='mira_image'>
          <img src="static/images/imagenet_d_samples.png" width="800px" height="2100px" />
        </div>

        <div class="content has-text-justified">
          <p>
            The ImageNet-D test set. Each group of images is generated with the same object and nuisance, such as background, texture, and material. For each group of images, the ground truth label is color green, while the predicted categories by CLIP (ViT-L/14) on each image are in black. Leveraging diffusion models for image generation, we can create a test set with diverse combinations of objects and nuisances. For example, the top left corner shows a bench in the swimming pool background. Interestingly, CLIP (ViT-L/14) recognizes the bench in this image as swimming trunks.
          </p>
        </div>
      </div>
    </div>
  </div>
  
    <!--/ Dataset samples. -->

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="one">
          <div class="two" id='mira_image'>
          <img src="static/images/framework.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            ImageNet-D creation framework. ImageNet-D is created by first combining various object categories and nuisances, including background, texture, and material. To make the test set challenging, we only keep the hard images from the large pool that commonly make multiple surrogate models fail to predict the correct object label. The test set is then refined through human verification to ensure the images are valid, single-class, and high-quality, making ImageNet-D suitable for evaluating the robustness of different neural networks.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Method. -->

     <!-- Result. -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="one">
          <div class="two" id='mira_image'>
          <img src="static/images/imagenet.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            Model accuracy on ImageNet vs.  ImageNet-D. Each data point corresponds to one tested model. The plots reveal  that there is a significant accuracy drop from ImageNet to our new test set ImageNet-D. As the model's accuracy on ImageNet increases, the accuracy on ImageNet-D is also higher. These results show the effectiveness of ImageNet-D to evaluate the robustness of neural networks.
          </p>
        </div>
      </div>
    </div>
      <!--/ Result. -->
    </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2024imagenet_d,
  author    = {Zhang, Chenshuang and Pan, Fei and Kim, Junmo and Kweon, In So and Mao, Chengzhi},
  title     = {ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object},
  journal   = {CVPR},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
          Thanks to <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for the source code of the nice website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
